{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient_Descent_linear_reg.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kWSNe8u06wUX","colab_type":"text"},"source":["# Linear Regression using Gradient Descent\n","\n","\n","### For this exercise we will use the Student performance data set. \n","This code and dataset was taken from [Siraj Raval's repository](https://github.com/llSourcell/linear_regression_live/blob/master/demo.py)"]},{"cell_type":"markdown","metadata":{"id":"f8HvP4wCoyG9","colab_type":"text"},"source":["### About the dataset:\n","The dataset contains the amount of hours studied and the correspondent grade earned by a student."]},{"cell_type":"code","metadata":{"id":"64TJhef2684s","colab_type":"code","colab":{}},"source":["#import libraries\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6cLENr4j9-Q","colab_type":"text"},"source":["### We compute the Gradient or Partial derivative by the following formula:"]},{"cell_type":"markdown","metadata":{"id":"0CWX2XC2jz5j","colab_type":"text"},"source":["<img src=\"https://spin.atomicobject.com/wp-content/uploads/linear_regression_gradient1.png\" height=\"170\">\n"]},{"cell_type":"code","metadata":{"id":"bWs9aziTDDWU","colab_type":"code","colab":{}},"source":["def step_gradient(b_current, m_current, points, learning_rate):\n","  b_gradient = 0\n","  m_gradient = 0\n","  N = float(len(points))\n","  for i in range(0, len(points)):\n","    x = points[i,0]\n","    y = points[i,1]\n","    b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n","    m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n","  new_b = b_current - (learning_rate * b_gradient)\n","  new_m = m_current - (learning_rate * m_gradient)\n","  return [new_b, new_m]\n","                "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lilnlrFDCl-U","colab_type":"code","colab":{}},"source":["def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n","  b = starting_b\n","  m = starting_m\n","  \n","  for i in range(num_iterations):\n","    b, m = step_gradient(b, m, array(points), learning_rate)\n","  return [b,m]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iu-7sZVeHYEL","colab_type":"text"},"source":["### We calculate the loss function using the Mean Squared Error given by the following formula:"]},{"cell_type":"markdown","metadata":{"id":"krO06h5ZG6UC","colab_type":"text"},"source":["<img src=\"https://spin.atomicobject.com/wp-content/uploads/linear_regression_error1.png \" height=\"130\">\n","\n"]},{"cell_type":"code","metadata":{"id":"HZdkRW_gDQ_4","colab_type":"code","colab":{}},"source":["def compute_error_for_given_points(b, m, points):\n","  total_error = 0\n","  for i in range(0, len(points)):\n","    x = points[i,0]\n","    y = points[i,1]\n","    total_error += (y - (m*x + b))**2\n","  return total_error / float(len(points))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SlfsssDZlJyp","colab_type":"text"},"source":["###Set initial parameters:"]},{"cell_type":"code","metadata":{"id":"bf3XsvsMh8z1","colab_type":"code","colab":{}},"source":["#load dataset\n","points = np.genfromtxt(\"https://raw.githubusercontent.com/llSourcell/linear_regression_live/master/data.csv\", delimiter=\",\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RL1nn8FW_m-R","colab_type":"code","colab":{}},"source":["#set learning rate\n","learning_rate = 0.0001"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eyAf3rdnCLeO","colab_type":"code","colab":{}},"source":["#slope formula:: y = mx + b\n","initial_b = 0\n","initial_m = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aEMgTR5OCSLh","colab_type":"code","colab":{}},"source":["#set number of iteration\n","num_iterations = 1000"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nA9h1ealVcA","colab_type":"text"},"source":["### Run Gradient descent:"]},{"cell_type":"code","metadata":{"id":"Ea5JMegWfrsD","colab_type":"code","outputId":"d361c5af-65e6-48de-9674-1855a7a0a449","executionInfo":{"status":"error","timestamp":1587251996290,"user_tz":360,"elapsed":940,"user":{"displayName":"Sara Iris Garcia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8ubo60uFw8TVu0LHA7Oo-ro2pALp0z5D5_vXh7g=s64","userId":"00232496476930050805"}},"colab":{"base_uri":"https://localhost:8080/","height":135}},"source":["# run gradient descent with initial paramenters\n","print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_given_points(initial_b, initial_m, points)))\n","print(\"Running...\")\n","[b,m] = gradient_descent_runner(dataset, initial_b, initial_m, learning_rate, num_iterations))\n","print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_given_points(b, m, points)))"],"execution_count":15,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-0dfe20b789ad>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    [b,m] = gradient_descent_runner(dataset, initial_b, initial_m, learning_rate, num_iterations))\u001b[0m\n\u001b[0m                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","metadata":{"id":"Zb_SggLWm_ex","colab_type":"text"},"source":["###Conclusion:\n","The local minima was found with the slope = 1.477 and an intercept = 0.0889"]}]}