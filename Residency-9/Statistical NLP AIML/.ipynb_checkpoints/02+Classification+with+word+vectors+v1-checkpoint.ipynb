{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3031,
     "status": "ok",
     "timestamp": 1615708917424,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "45DThpU31Wv9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 3029,
     "status": "ok",
     "timestamp": 1615708917427,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "h-j-JIIF1WwE",
    "outputId": "66550134-a295-44cf-a2a3-99fcc8223a25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZhwpoi_1WwH"
   },
   "source": [
    "### Loading the training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1615709164987,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "a4nQXFg01WwI",
    "outputId": "cb5f2e79-891c-416f-f6b5-aa2d9dd71280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#This is needed only if you have uploaded data to Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "train_data = \"/gdrive/My Drive/Statistical NLP AIML/r8-train-all-terms.txt\"\n",
    "test_data = \"/gdrive/My Drive/Statistical NLP AIML/r8-test-all-terms.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1314,
     "status": "ok",
     "timestamp": 1615709166931,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "quo1ehc-1WwL",
    "outputId": "5a0bff03-2f5d-4422-b584-220996aaf5f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 5485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "with open(train_data, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        label, text = line.split(\"\\t\")\n",
    "        X.append(text.split())\n",
    "        y.append(label)\n",
    "X, y = np.array(X), np.array(y)\n",
    "print (\"total examples %s\" % len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1135,
     "status": "ok",
     "timestamp": 1615709174665,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "0nSsgHjB1WwO",
    "outputId": "4b24ad75-3b58-464d-a15c-23f8d84e155c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 2189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = [], []\n",
    "with open(test_data, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        label, text = line.split(\"\\t\")\n",
    "        X_test.append(text.split())\n",
    "        y_test.append(label)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "print (\"total examples %s\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 812,
     "status": "ok",
     "timestamp": 1615709177886,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "V5bMdAh81WwT",
    "outputId": "dda0ae7b-2ddb-496c-985c-d984357f2507"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5485,), (2189,))"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1615709178232,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "YLueh8kS1WwW",
    "outputId": "7febffe5-7ddd-4d1c-e436-b0ac59eac42d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['acq', 'crude', 'earn', 'grain', 'interest', 'money-fx', 'ship',\n",
       "        'trade'], dtype='<U8'),\n",
       " array([1596,  253, 2840,   41,  190,  206,  108,  251]))"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4It5TpQ1Wwa"
   },
   "source": [
    "## Using NB methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TXYaHt01Wwa"
   },
   "source": [
    "First, reconstructring the string text for our vectorizers to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1159,
     "status": "ok",
     "timestamp": 1615709215866,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "H9c3MZaN1Wwb",
    "outputId": "33951907-cef2-4227-f848-a5d01ff8922b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "champion products ch approves stock split champion products inc said its board of directors approved a two for one stock split of its common shares for shareholders of record as of april the company also said its board voted to recommend to shareholders at the annual meeting april an increase in the authorized capital stock from five mln to mln shares reuter\n",
      "\n",
      "vieille montagne reports loss dividend nil year net loss after exceptional charges mln francs vs profit mln exceptional provisions for closure of viviez electrolysis plant mln francs vs exceptional gain mln sales and services billion francs vs billion proposed net dividend on ordinary shares nil vs francs company s full name is vieille montagne sa vmnb br reuter\n"
     ]
    }
   ],
   "source": [
    "X_text = [\" \".join(val) for val in X]\n",
    "print(X_text[0] + \"\\n\")\n",
    "X_test_text = [\" \".join(val) for val in X_test]\n",
    "print(X_test_text[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2qJ5drM1Wwe"
   },
   "source": [
    "#### Using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1615709217225,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "zHjDK8qT1Wwf"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1615709217939,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "ZxtKcrRh1Wwj",
    "outputId": "2c30a381-9a04-4c8b-8661-caa147d38706"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english',max_features=5000)\n",
    "vect.fit(X_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGAExdk81Wwm"
   },
   "source": [
    "Applying the vectorizer to our test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 1332,
     "status": "ok",
     "timestamp": 1615709220894,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "nRoQPGAL1Wwm"
   },
   "outputs": [],
   "source": [
    "X_train_transformed = vect.transform(X_text)\n",
    "X_test_transformed =vect.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1615709220896,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "fwJbpMtq1Wwp",
    "outputId": "d68abe7a-aaa1-46d4-c204-62a27fefbb30",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('champion', 759),\n",
       " ('products', 3488),\n",
       " ('approves', 264),\n",
       " ('stock', 4337),\n",
       " ('split', 4273),\n",
       " ('said', 3974),\n",
       " ('board', 532),\n",
       " ('directors', 1327),\n",
       " ('approved', 263),\n",
       " ('common', 895)]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the vocabulary\n",
    "list(vect.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bumLEp-L1Wwr"
   },
   "source": [
    "### Using Bernoulli NB first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1615709224976,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "h4ybpZKF1Wws"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1063,
     "status": "ok",
     "timestamp": 1615709225962,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "JBCGVuJ31Wwv",
    "outputId": "a9c90447-5b0d-43ed-b7f7-97e9c2af1e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8736554238833182\n",
      "Test accuracy:  0.8688899040657835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_transformed,y)\n",
    "\n",
    "# predict class\n",
    "pred_train_ys = bnb.predict(X_train_transformed)\n",
    "pred_test_ys = bnb.predict(X_test_transformed)\n",
    "\n",
    "# accuracy\n",
    "print(\"Train accuracy: \", accuracy_score(y, pred_train_ys))\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, pred_test_ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWfWQB6v1Ww1"
   },
   "source": [
    "### Using Multinomial NB\n",
    " - We expect this to work very well, giving high performance in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1615709228398,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "lH0bySCB1Ww2"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 966,
     "status": "ok",
     "timestamp": 1615709228734,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "YslTT6iu1Ww5",
    "outputId": "732f7baa-d75b-4fca-9fde-df9d8c8840e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.968094804010939\n",
      "Test accuracy:  0.9657377798081316\n"
     ]
    }
   ],
   "source": [
    "#fit on training data\n",
    "mnb.fit(X_train_transformed, y)\n",
    "\n",
    "# predict class\n",
    "pred_train_ys = mnb.predict(X_train_transformed)\n",
    "pred_test_ys = mnb.predict(X_test_transformed)\n",
    "\n",
    "# accuracy\n",
    "print(\"Train accuracy: \", accuracy_score(y, pred_train_ys))\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, pred_test_ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7utCe2-1Ww8"
   },
   "source": [
    " - As expected, this performed really well\n",
    " - Remember that we used 5000 features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTvHeUeh1Ww9"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RgVq6GQ1Ww-"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ8bYZuc1Ww-"
   },
   "source": [
    "## Using our word embeddings approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px6dXRxd1Ww_"
   },
   "source": [
    " We have two options here:\n",
    " \n",
    "1. Use pre-trained word vectors (Glove)\n",
    "\n",
    "2. Train our own vectors\n",
    "\n",
    "We'll explore both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOkiQ62E1Ww_"
   },
   "source": [
    "### Loading the pre-trained GloVe word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1615709243943,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgqVHJ9o6-7idGiRbNNd4iVc1VjzvuPnwmAnvEtD6U=s64",
      "userId": "03603580465490055794"
     },
     "user_tz": -330
    },
    "id": "mY9UFZDB1WxA"
   },
   "outputs": [],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove_input_file = 'glove.6B.100d.txt'\n",
    "# word2vec_output_file = 'glove.6B.100d.w2vformat.txt'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDpblBuo1WxE",
    "outputId": "fbb26806-7d4a-4191-9e0b-772fe36fe6f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.200d.txt'\n",
    "word2vec_output_file = 'glove.6B.200d.w2vformat.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4Czp49g1WxI"
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"glove.6B.200d.w2vformat.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdDnSDFq1WxL"
   },
   "source": [
    "### Sentence vector by averaging word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLvO_3ID1WxL"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Anq3tvGV1WxN"
   },
   "outputs": [],
   "source": [
    "def sent_vec(sent):\n",
    "    wv_res = np.zeros(glove_model.vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in glove_model:\n",
    "            ctr += 1\n",
    "            wv_res += glove_model[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    #return (wv_res, ctr)\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0jnOS451WxQ"
   },
   "outputs": [],
   "source": [
    "train_doc_vecs = []\n",
    "for doc in X:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    train_doc_vecs.append(sent_vec(doc_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dVdJIN31WxV"
   },
   "outputs": [],
   "source": [
    "test_doc_vecs = []\n",
    "for doc in X_test:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    test_doc_vecs.append(sent_vec(doc_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z-QpIMg1WxZ"
   },
   "source": [
    "### Building a predictive model on the averaged word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaa0mZrE1WxZ"
   },
   "source": [
    "#### Using a 'simple' logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyuvoBlM1WxZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVDxHLW21Wxb",
    "outputId": "01e1ebea-af0d-4493-ac3a-5120beae5075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=3.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(penalty=\"l1\", random_state=42, C = 3.5)\n",
    "logreg.fit(train_doc_vecs,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W_m0pzd1Wxe",
    "outputId": "91e06520-6964-4015-b28d-6e62df6e0b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.978122151322\n",
      "Test accuracy:  0.968021927821\n"
     ]
    }
   ],
   "source": [
    "pred_train_ys = logreg.predict(train_doc_vecs)\n",
    "pred_test_ys = logreg.predict(test_doc_vecs)\n",
    "print(\"Train accuracy: \", accuracy_score(pred_train_ys, y))\n",
    "print(\"Test accuracy: \", accuracy_score(pred_test_ys, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJylRuQY1Wxg"
   },
   "source": [
    "### Training our own wordvectors on the data\n",
    "We'll create a combined text file to train our word vectors - more data is better. Although in this case we would still have just 7.7K instances to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycC4F0df1Wxg",
    "outputId": "0deafec9-6f67-426e-f923-cc5b354af716"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7674"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_comb = np.concatenate([X,X_test])\n",
    "len(X_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClMRAKJJ1Wxk",
    "outputId": "6307d87a-963a-461d-bfa2-585ef432dafe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fhlbb', 'changes', 'short', 'term', 'discount', 'note', 'rates', 'the', 'federal', 'home', 'loan', 'bank', 'board', 'adjusted', 'the', 'rates', 'on', 'its', 'short', 'term', 'discount', 'notes', 'as', 'follows', 'maturity', 'new', 'rate', 'old', 'rate', 'maturity', 'days', 'pct', 'pct', 'days', 'days', 'pct', 'pct', 'days', 'days', 'pct', 'pct', 'days', 'days', 'pct', 'pct', 'days', 'days', 'pct', 'pct', 'days', 'reuter']\n"
     ]
    }
   ],
   "source": [
    "print(X_comb[6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ecqctvys1Wxn"
   },
   "outputs": [],
   "source": [
    "w2v = word2vec.Word2Vec(X_comb, window=2, min_count=2, sg = 1, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueskfkXy1Wxp",
    "outputId": "4e6c47ae-f205-4f41-82fb-2fc14678f2f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('near', 0.7925878763198853),\n",
       " ('prospects', 0.7876253128051758),\n",
       " ('method', 0.7740976810455322),\n",
       " ('developments', 0.7626558542251587),\n",
       " ('foreseeable', 0.7616651058197021)]"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(\"future\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp5NW5_n1Wxs"
   },
   "source": [
    "#### Sentence vectors by averaging vectors for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "2YLQ5i0B1Wxs"
   },
   "outputs": [],
   "source": [
    "def sent_vec_w2v(sent):\n",
    "    wv_res = np.zeros(w2v.vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in w2v:\n",
    "            ctr += 1\n",
    "            wv_res += w2v[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upjzC0f61Wxu"
   },
   "source": [
    "#### Getting the sentence vectors for the test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUY94-qB1Wxv"
   },
   "outputs": [],
   "source": [
    "train_doc_vecs = []\n",
    "for doc in X:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    train_doc_vecs.append(sent_vec_w2v(doc_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKT17EBq1Wxx"
   },
   "outputs": [],
   "source": [
    "test_doc_vecs = []\n",
    "for doc in X_test:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    test_doc_vecs.append(sent_vec_w2v(doc_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSL7df4e1Wxz"
   },
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFbS6hDe1Wxz"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9FVkfwM1Wx3"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty=\"l1\", random_state=42, C = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8cUc7Hk1Wx9",
    "outputId": "ec5a3317-2ad4-47e9-ee2e-ffaa1d3d147f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(train_doc_vecs,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdKbLVF_1WyA",
    "outputId": "d1cac396-9575-47df-b95a-e51729a77964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.971011850501\n",
      "Test accuracy:  0.966194609411\n"
     ]
    }
   ],
   "source": [
    "pred_train_ys = logreg.predict(train_doc_vecs)\n",
    "pred_test_ys = logreg.predict(test_doc_vecs)\n",
    "print(\"Train accuracy: \", accuracy_score(pred_train_ys, y))\n",
    "print(\"Test accuracy: \", accuracy_score(pred_test_ys, y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02+Classification+with+word+vectors+v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
