{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrm0IqEPPHEW",
    "outputId": "510cdf6f-7a3b-4b6e-ccec-3738c32e8ad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#connect to google drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56kCBRVpQSHS",
    "outputId": "13209be9-3057-48c0-a929-90a7df9508be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot\n"
     ]
    }
   ],
   "source": [
    "cd '/content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqNkf-qYQjOr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apNGkqJ4QjrW"
   },
   "source": [
    "# Load JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Dy1c6q8JQc7x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JU9I6v1tRRDt"
   },
   "outputs": [],
   "source": [
    "with open(\"GL Bot.json\") as file:\n",
    "  Botpattern = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mIUvuiFfS7tr"
   },
   "outputs": [],
   "source": [
    "lsttag = []\n",
    "lstpattern =[]\n",
    "lstresponse =[]\n",
    "dfs =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isoVkUPuewow"
   },
   "source": [
    "Lets read the JSON file and store the details in dataframe. \n",
    "The data in dataframe will be used for NLP processing and model buidling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ygRZsJQAXhtN"
   },
   "outputs": [],
   "source": [
    "for intent in Botpattern[\"intents\"] :\n",
    "  tagval = intent[\"tag\"]                \n",
    "  res = intent[\"responses\"]\n",
    "\n",
    "  for pattern in intent[\"patterns\"] :\n",
    "    lstpattern.append(pattern)\n",
    "    lsttag.append(tagval)\n",
    "    lstresponse.append(res[0])\n",
    "  df = pd.DataFrame({'tag' : lsttag, 'response' : lstresponse, 'pattern' :lstpattern}) \n",
    "  dfs.append(df)\n",
    "  lstpattern.clear()\n",
    "  lsttag.clear()\n",
    "  lstresponse.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qejJFREpeGb5",
    "outputId": "655d48a1-e39e-44d0-b3dd-6fc9bd00c1f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BiemeFvLWZhS"
   },
   "outputs": [],
   "source": [
    "#Lets merge data of all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vGjkU8DuWdoJ"
   },
   "outputs": [],
   "source": [
    "finaldf = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kI5QWjv1fkZ_",
    "outputId": "98cf29b4-95d2-46c6-96a8-943ed5fa2634"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRMITKCGfma6",
    "outputId": "21587188-eb40-4f08-92bb-fe6752f00121"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SL         29\n",
       "NN         24\n",
       "Intro      20\n",
       "Exit       16\n",
       "Olympus    13\n",
       "Ticket      9\n",
       "Profane     9\n",
       "Bot         8\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fNw6W1ZVjGY_"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "labels = le.fit_transform(finaldf['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sYPJg_2Ajekt"
   },
   "outputs": [],
   "source": [
    "finaldf['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "LrSnEiVCj_vw",
    "outputId": "c73e888c-0321-47b9-b4be-2f0636f641d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>response</th>\n",
       "      <th>pattern</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Intro</td>\n",
       "      <td>Hello! how can i help you ?</td>\n",
       "      <td>aifl batch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Intro</td>\n",
       "      <td>Hello! how can i help you ?</td>\n",
       "      <td>i am learner from</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Exit</td>\n",
       "      <td>I hope I was able to assist you, Good Bye</td>\n",
       "      <td>great help</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Intro</td>\n",
       "      <td>Hello! how can i help you ?</td>\n",
       "      <td>aiml batch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exit</td>\n",
       "      <td>I hope I was able to assist you, Good Bye</td>\n",
       "      <td>later</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tag                                   response            pattern  labels\n",
       "12  Intro                Hello! how can i help you ?         aifl batch       2\n",
       "9   Intro                Hello! how can i help you ?  i am learner from       2\n",
       "13   Exit  I hope I was able to assist you, Good Bye         great help       1\n",
       "11  Intro                Hello! how can i help you ?         aiml batch       2\n",
       "4    Exit  I hope I was able to assist you, Good Bye              later       1"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uDu0t_dvkG2U"
   },
   "outputs": [],
   "source": [
    "#Save the dataframe\n",
    "from numpy import asarray, save\n",
    "save('/content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/chatdata.npy',finaldf.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trUuQ-RXhiF7"
   },
   "source": [
    "# Solution Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FojQpCpKhm6z"
   },
   "source": [
    "The chatbot data primary consists of\n",
    "- pattern : This is corpus and will act as input feature\n",
    "- tag : This is target variable i.e. output which we need to predict. \n",
    "- Response : This will be that value corresponding to predicted tag. The value will be extracted from the dataframe which will have mapping of tag -> Response\n",
    "\n",
    "Model - Build and test differtn classfier like logistic regression, SVM etc and select that model which gives maximum accuracy. Save the model and load the in runtime. Also load dataframe (of tag-> responses). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF-PD5uypkSd"
   },
   "source": [
    "# Assumptions and Exclusions\n",
    "Assumptions -\n",
    "1) The corpus is will not contain accented characters\n",
    "2) The size of corpus is limited.\n",
    "Exclusions\n",
    "1) Preprocessing methods like removing stop words, stemming or lemmatisation are not applied as most the patterns (questions) contain stop words e.g. \"I am from from\", \"what is up\", \"is anyone there\" etc. if we remove stop words then document will not have any words and thus features and also does not make sense\n",
    "2) One of the goal of preprocessing stpes like remove stop words etc is to reduce dimentionality i.e. reduce number of words (features). In this case the corpus is limited and number of words are still managible even without removing stop words.\n",
    "Assumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmiJbPWkm9Sw"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IFbZptkHg7NL"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7snmDY6Ynh6f"
   },
   "outputs": [],
   "source": [
    "def Remove_SpecialChars(text, remove_digits=False):\n",
    "  pattern = r'[^a-zA-z\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "  text = re.sub(pattern, ' ', text)  # remove special characters and numbers also\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yppCd_V-nvGf",
    "outputId": "77b55f01-9815-4b96-e94c-1a643e51a2b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "szrRHG36n2qd"
   },
   "outputs": [],
   "source": [
    "def RemoveStopWords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-yCkJoNZn7Qo"
   },
   "outputs": [],
   "source": [
    "def Stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tPs4jcXcoBgX"
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7IK1JrAxoIbk"
   },
   "outputs": [],
   "source": [
    "def Lemmatization(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "y5DyAH5-oL_u"
   },
   "outputs": [],
   "source": [
    "# Lets bundle above functions in one function\n",
    "def TextProcessing(text) :\n",
    "  txt = text.lower()\n",
    "  txt = RemoveStopWords(txt)\n",
    "  txt = Remove_SpecialChars(txt)\n",
    "  #txt = Stemming(txt)\n",
    "  #txt = Lemmatization(txt)\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "kljTfrNhoWOD"
   },
   "outputs": [],
   "source": [
    "finaldf['Processedpattern'] = finaldf['pattern'].apply(lambda x : TextProcessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Q_nLQh8yoqgl",
    "outputId": "09948cd5-aba5-4f6e-bb1c-011ceebbfc3b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>Processedpattern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>artificial intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unable to understand neural nets</td>\n",
       "      <td>unable understand neural nets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hyper parameters</td>\n",
       "      <td>hyper parameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>too good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>forward propagation</td>\n",
       "      <td>forward propagation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             pattern               Processedpattern\n",
       "8            artificial intelligence        artificial intelligence\n",
       "6   unable to understand neural nets  unable understand neural nets\n",
       "28                  hyper parameters               hyper parameters\n",
       "14                          too good                           good\n",
       "17               forward propagation            forward propagation"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf[['pattern','Processedpattern']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3_EL-jVVzveL"
   },
   "outputs": [],
   "source": [
    "Nooflabels = pd.unique(finaldf['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-LsfimKz62K",
    "outputId": "1164c84d-e137-4683-fc20-ef867b326d8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Nooflabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "eXHOR49gerbU"
   },
   "outputs": [],
   "source": [
    "classes = pd.unique(finaldf['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5T1bzClLev6S",
    "outputId": "4db87803-7936-4085-e466-cf7b598ed6eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Intro', 'Exit', 'Olympus', 'SL', 'NN', 'Bot', 'Profane', 'Ticket'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QW3tPHbvBJE"
   },
   "source": [
    "# Build Classifier Model - To preduct tag(response) for given pattern (questions) based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "tLDtM4pqpNYA"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Split data into 80:20 :: train : test \n",
    "X_train, X_test, y_train, y_test = train_test_split(finaldf.Processedpattern.values,\n",
    "                                                    finaldf.labels.values,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=24\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "g8embtBCva5A"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(ngram_range=(1,2)) \n",
    "x_train1 = vect.fit_transform(X_train)\n",
    "x_test1 = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9SGb00px9h9",
    "outputId": "d652bd22-ec4f-42c1-c7b5-1f7f05447694"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUst46lgxMnz"
   },
   "source": [
    "Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "vKNcw_Ebvtxk"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticmodel = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6xLk_bjv3xp",
    "outputId": "e7501531-9a4b-41c1-a449-71a07248fa59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticmodel.fit(x_train1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "VTNljC4twcoh"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mru1UgFWMMAJ"
   },
   "source": [
    "Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "eTBYwZiMwhBR"
   },
   "outputs": [],
   "source": [
    "predicted_labels_train = logisticmodel.predict(x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffItIMYpwokD",
    "outputId": "a6d2bb48-edae-4334-d4e7-284b4f7d7c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9901960784313726\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_train,predicted_labels_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wpsxp1S0wy2f"
   },
   "source": [
    "Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "hH70UNavwvWP"
   },
   "outputs": [],
   "source": [
    "predicted_labels_test = logisticmodel.predict(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98oazTaJw47c",
    "outputId": "7d1e72c8-f096-46a0-bd00-d28892510245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,predicted_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "LTO-BTtrMRx6"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "import pickle\n",
    "pickle.dump(logisticmodel,open('/content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/logisticmodel.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nts3WgMhxQXj"
   },
   "source": [
    "SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "pu4Y1ll_xAe0"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "svSblDpNxVA0"
   },
   "outputs": [],
   "source": [
    "svcmodel = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAzZoXstxZ2m",
    "outputId": "ae942623-1fa9-468d-b3bd-8a7806cdac7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcmodel.fit(x_train1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "5MiLLrLjxf0E"
   },
   "outputs": [],
   "source": [
    "predicted_labels_train = svcmodel.predict(x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0ODMU6nxoiq",
    "outputId": "4bf5e584-9e0c-4f88-9b08-a2e7f2eea516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7941176470588235\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_train,predicted_labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyozyHHExo4-",
    "outputId": "27252ba1-6fb5-4ae5-9aae-65091e3c2b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_test = logisticmodel.predict(x_test1)\n",
    "print(accuracy_score(y_test,predicted_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "1FMUzxXUNkMu"
   },
   "outputs": [],
   "source": [
    "#Save model\n",
    "pickle.dump(svcmodel,open('/content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/svcmodel.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCKLSF6oyBpH"
   },
   "source": [
    "DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "ZMBSPmNfxwi9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "ocldnUwcyHPX"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "RGUugi-k1zjl"
   },
   "outputs": [],
   "source": [
    "x_train1_arr = x_train1.toarray()\n",
    "x_test1_arr = x_test1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGFRb97o50yW",
    "outputId": "9606ad65-1a77-48bc-e710-da708e48c9d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5fiPid75wg-",
    "outputId": "dc07233d-3a19-42d0-c70a-b6611ff928c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 190)"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbcpsKGN6fvx",
    "outputId": "f116273f-cfb4-4d2c-b433-44b327c85baa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 190)"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test1_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5Mv9Faq6DQC",
    "outputId": "9b56914f-c3e0-4e07-a4ac-6bda90286b1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "Dhdg2bEY-V00"
   },
   "outputs": [],
   "source": [
    "y_train_onehot = tf.keras.utils.to_categorical(y_train)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNzRljZZ-sLr",
    "outputId": "c0876185-771f-4f13-dac2-647c8e1f42aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 8)"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "Akbk43SuyK7P"
   },
   "outputs": [],
   "source": [
    "#Add hidden layers\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu', input_shape=(len(vect.vocabulary_),)))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Dense(30, activation='relu'))\n",
    "\n",
    "#Add Output layer\n",
    "#model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(len(Nooflabels), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "ANCwy0ofyejD"
   },
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "FdWbVvZXPqT1"
   },
   "outputs": [],
   "source": [
    "model_checkpoint=tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5', \n",
    "                                                    save_best_only=True, \n",
    "                                                    monitor='val_accuracy', \n",
    "                                                    mode='max', \n",
    "                                                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTjJIFfdy3tx",
    "outputId": "db3eacf7-11fe-42d6-ade6-cbf30a9097d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "6/6 [==============================] - 1s 49ms/step - loss: 2.0885 - accuracy: 0.1227 - val_loss: 2.0740 - val_accuracy: 0.1923\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.19231, saving model to /content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5\n",
      "Epoch 2/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.0692 - accuracy: 0.1663 - val_loss: 2.0624 - val_accuracy: 0.1154\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.19231\n",
      "Epoch 3/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.0438 - accuracy: 0.2543 - val_loss: 2.0508 - val_accuracy: 0.2308\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.19231 to 0.23077, saving model to /content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5\n",
      "Epoch 4/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2.0400 - accuracy: 0.2808 - val_loss: 2.0381 - val_accuracy: 0.3077\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.23077 to 0.30769, saving model to /content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5\n",
      "Epoch 5/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.0265 - accuracy: 0.2036 - val_loss: 2.0243 - val_accuracy: 0.3462\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.30769 to 0.34615, saving model to /content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5\n",
      "Epoch 6/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2.0110 - accuracy: 0.2725 - val_loss: 2.0055 - val_accuracy: 0.3462\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.34615\n",
      "Epoch 7/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2.0176 - accuracy: 0.2396 - val_loss: 1.9830 - val_accuracy: 0.3462\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.34615\n",
      "Epoch 8/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.9525 - accuracy: 0.3230 - val_loss: 1.9576 - val_accuracy: 0.3462\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.34615\n",
      "Epoch 9/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.9491 - accuracy: 0.3050 - val_loss: 1.9310 - val_accuracy: 0.3462\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.34615\n",
      "Epoch 10/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.9280 - accuracy: 0.2755 - val_loss: 1.9044 - val_accuracy: 0.3462\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.34615\n",
      "Epoch 11/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.9115 - accuracy: 0.2990 - val_loss: 1.8752 - val_accuracy: 0.3462\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.34615\n",
      "Epoch 12/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.8612 - accuracy: 0.3798 - val_loss: 1.8425 - val_accuracy: 0.3462\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.34615\n",
      "Epoch 13/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.8538 - accuracy: 0.2914 - val_loss: 1.8073 - val_accuracy: 0.3846\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.34615 to 0.38462, saving model to /content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5\n",
      "Epoch 14/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.8204 - accuracy: 0.3507 - val_loss: 1.7735 - val_accuracy: 0.3846\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.38462\n",
      "Epoch 15/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.7858 - accuracy: 0.3814 - val_loss: 1.7410 - val_accuracy: 0.4231\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.38462 to 0.42308, saving model to /content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5\n",
      "Epoch 16/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.7826 - accuracy: 0.3215 - val_loss: 1.7101 - val_accuracy: 0.4231\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.42308\n",
      "Epoch 17/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.7236 - accuracy: 0.3826 - val_loss: 1.6760 - val_accuracy: 0.4231\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.42308\n",
      "Epoch 18/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.6778 - accuracy: 0.3897 - val_loss: 1.6488 - val_accuracy: 0.4231\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.42308\n",
      "Epoch 19/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.6277 - accuracy: 0.3922 - val_loss: 1.6262 - val_accuracy: 0.4231\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.42308\n",
      "Epoch 20/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.6313 - accuracy: 0.3403 - val_loss: 1.6042 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.42308 to 0.53846, saving model to /content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5\n",
      "Epoch 21/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.5136 - accuracy: 0.4743 - val_loss: 1.5817 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.53846\n",
      "Epoch 22/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.4474 - accuracy: 0.4609 - val_loss: 1.5628 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.53846\n",
      "Epoch 23/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.4899 - accuracy: 0.5100 - val_loss: 1.5399 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.53846\n",
      "Epoch 24/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.4212 - accuracy: 0.5274 - val_loss: 1.5193 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.53846 to 0.57692, saving model to /content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5\n",
      "Epoch 25/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.3980 - accuracy: 0.5154 - val_loss: 1.4996 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.57692\n",
      "Epoch 26/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.3777 - accuracy: 0.5510 - val_loss: 1.4793 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.57692\n",
      "Epoch 27/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.3162 - accuracy: 0.5444 - val_loss: 1.4628 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.57692\n",
      "Epoch 28/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.2723 - accuracy: 0.6282 - val_loss: 1.4499 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.57692\n",
      "Epoch 29/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.2092 - accuracy: 0.6374 - val_loss: 1.4347 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.57692\n",
      "Epoch 30/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.1385 - accuracy: 0.6139 - val_loss: 1.4212 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.57692\n",
      "Epoch 31/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.1714 - accuracy: 0.6770 - val_loss: 1.4122 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.57692\n",
      "Epoch 32/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.0216 - accuracy: 0.7330 - val_loss: 1.4026 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.57692\n",
      "Epoch 33/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.1181 - accuracy: 0.7740 - val_loss: 1.3854 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.57692\n",
      "Epoch 34/80\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.9623 - accuracy: 0.7473 - val_loss: 1.3785 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.57692\n",
      "Epoch 35/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.9810 - accuracy: 0.7983 - val_loss: 1.3747 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.57692\n",
      "Epoch 36/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.9061 - accuracy: 0.7646 - val_loss: 1.3679 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.57692\n",
      "Epoch 37/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.8811 - accuracy: 0.7417 - val_loss: 1.3660 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.57692\n",
      "Epoch 38/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.7930 - accuracy: 0.8655 - val_loss: 1.3601 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.57692\n",
      "Epoch 39/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.7895 - accuracy: 0.8468 - val_loss: 1.3670 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.57692\n",
      "Epoch 40/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6863 - accuracy: 0.8574 - val_loss: 1.3588 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.57692\n",
      "Epoch 41/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.7138 - accuracy: 0.8334 - val_loss: 1.3665 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.57692\n",
      "Epoch 42/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.7003 - accuracy: 0.7923 - val_loss: 1.3712 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.57692\n",
      "Epoch 43/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.6920 - accuracy: 0.7882 - val_loss: 1.3865 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.57692\n",
      "Epoch 44/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.6589 - accuracy: 0.8496 - val_loss: 1.3918 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.57692\n",
      "Epoch 45/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.6450 - accuracy: 0.8467 - val_loss: 1.3886 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.57692\n",
      "Epoch 46/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5367 - accuracy: 0.8771 - val_loss: 1.3815 - val_accuracy: 0.5385\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.57692\n",
      "Epoch 47/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5605 - accuracy: 0.8683 - val_loss: 1.4048 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.57692\n",
      "Epoch 48/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5111 - accuracy: 0.8610 - val_loss: 1.4487 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00048: val_accuracy improved from 0.57692 to 0.61538, saving model to /content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5\n",
      "Epoch 49/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.5232 - accuracy: 0.8723 - val_loss: 1.4658 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.61538\n",
      "Epoch 50/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.5197 - accuracy: 0.9023 - val_loss: 1.4559 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.61538\n",
      "Epoch 51/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4743 - accuracy: 0.9175 - val_loss: 1.4467 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.61538\n",
      "Epoch 52/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.4837 - accuracy: 0.8520 - val_loss: 1.4281 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.61538\n",
      "Epoch 53/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.3956 - accuracy: 0.9076 - val_loss: 1.4155 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.61538\n",
      "Epoch 54/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4081 - accuracy: 0.9013 - val_loss: 1.4196 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.61538\n",
      "Epoch 55/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.4917 - accuracy: 0.8469 - val_loss: 1.4338 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.61538\n",
      "Epoch 56/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3540 - accuracy: 0.9442 - val_loss: 1.4591 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.61538\n",
      "Epoch 57/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3566 - accuracy: 0.9287 - val_loss: 1.4895 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.61538\n",
      "Epoch 58/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.2840 - accuracy: 0.9610 - val_loss: 1.5141 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.61538\n",
      "Epoch 59/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3627 - accuracy: 0.9175 - val_loss: 1.5308 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.61538\n",
      "Epoch 60/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.3583 - accuracy: 0.9592 - val_loss: 1.5297 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.61538\n",
      "Epoch 61/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.2883 - accuracy: 0.9455 - val_loss: 1.5257 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.61538\n",
      "Epoch 62/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.3357 - accuracy: 0.8665 - val_loss: 1.5213 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.61538\n",
      "Epoch 63/80\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2965 - accuracy: 0.9599 - val_loss: 1.5523 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.61538\n",
      "Epoch 64/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.3016 - accuracy: 0.9143 - val_loss: 1.5605 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.61538\n",
      "Epoch 65/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.2654 - accuracy: 0.9467 - val_loss: 1.5591 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.61538\n",
      "Epoch 66/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.2559 - accuracy: 0.9383 - val_loss: 1.5748 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.61538\n",
      "Epoch 67/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.2586 - accuracy: 0.9520 - val_loss: 1.6049 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.61538\n",
      "Epoch 68/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.2529 - accuracy: 0.9490 - val_loss: 1.6307 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.61538\n",
      "Epoch 69/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.2750 - accuracy: 0.9222 - val_loss: 1.6552 - val_accuracy: 0.6154\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.61538\n",
      "Epoch 70/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.2503 - accuracy: 0.9472 - val_loss: 1.6540 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.61538\n",
      "Epoch 71/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.2426 - accuracy: 0.9689 - val_loss: 1.6381 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.61538\n",
      "Epoch 72/80\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.1649 - accuracy: 0.9697 - val_loss: 1.6307 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.61538\n",
      "Epoch 73/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.1904 - accuracy: 0.9520 - val_loss: 1.6629 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.61538\n",
      "Epoch 74/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.2649 - accuracy: 0.9479 - val_loss: 1.6867 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.61538\n",
      "Epoch 75/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.2551 - accuracy: 0.9312 - val_loss: 1.6930 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.61538\n",
      "Epoch 76/80\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.2235 - accuracy: 0.9605 - val_loss: 1.7144 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.61538\n",
      "Epoch 77/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.2401 - accuracy: 0.9629 - val_loss: 1.7375 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.61538\n",
      "Epoch 78/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.1631 - accuracy: 0.9749 - val_loss: 1.7411 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.61538\n",
      "Epoch 79/80\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.2023 - accuracy: 0.9449 - val_loss: 1.7554 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.61538\n",
      "Epoch 80/80\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.1538 - accuracy: 0.9761 - val_loss: 1.7471 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.61538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fde912a4750>"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train1_arr, y_train_onehot,\n",
    "           validation_data=(x_test1_arr, y_test_onehot), \n",
    "           epochs=80, batch_size=20,callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfCjzwOmOzkF"
   },
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ITWZ9qzO6Qy"
   },
   "source": [
    "1) Logistic Regression \n",
    "   - Training Accuracy - 99%\n",
    "   - Test Accuracy     - 57 %\n",
    "2) SVM\n",
    "   - Training Accuracy - 7986 %\n",
    "   - Test Accuracy     - 57 %\n",
    "3) Nueral Network \n",
    "   - Training Accuracy - 88% \n",
    "   - Test Accuracy     - 61%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zL89I0sxTLph"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeThthYYTM2y"
   },
   "source": [
    "# Chat bot flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sct-oP1rTR79"
   },
   "source": [
    "The text accuracy provided by neural netwrok model is higher...so lets use ANN model for chat bot flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "FPQRl--tUacv"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "annmodelpredict = load_model('/content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/annmodel.h5')\n",
    "import pickle\n",
    "#modelpredict = pickle.load(open('/content/drive/MyDrive/LablFiles/Stat_NL_Project/chatbot/svcmodel.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "X57NT-pYfqXM"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "def clean_text(text): \n",
    "  tokens = tokenizer.tokenize(text)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "b9RLAy8YeSZD"
   },
   "outputs": [],
   "source": [
    "def bag_of_words(text, vocab): \n",
    "  tokens = clean_text(TextProcessing(text))\n",
    "  bow = [0] * len(vocab)\n",
    "  for w in tokens: \n",
    "    for idx, word in enumerate(vocab):\n",
    "      if word == w: \n",
    "        bow[idx] = 1\n",
    "  return np.array(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "5rxTlE36eald"
   },
   "outputs": [],
   "source": [
    "def pred_class(text, vocab, labels): \n",
    "  bow = bag_of_words(text, vocab)\n",
    "  result = annmodelpredict.predict(np.array([bow]))[0] #ANN\n",
    "  #result = modelpredict.predict(np.array([bow]))       #SVM model\n",
    "  thresh = 0.2\n",
    "  y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
    "\n",
    "  y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "  return_list = []\n",
    "  for r in y_pred:\n",
    "    return_list.append(labels[r[0]])\n",
    "  return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "Lv3n4XuF-d95"
   },
   "outputs": [],
   "source": [
    "def pred_class_modified(text, vocab, labels): \n",
    "  bow = bag_of_words(text, vocab)\n",
    "  result = annmodelpredict.predict(np.array([bow]))[0] #ANN\n",
    "  #result = modelpredict.predict(np.array([bow]))       #SVM model\n",
    "  lst = list(result)\n",
    "  max_val = max(lst)\n",
    "  index = lst.index(max_val)\n",
    "  return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdrPm3OHDgoR",
    "outputId": "4ee55dd1-4f90-4e73-824c-8457e6304f20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olympus\n"
     ]
    }
   ],
   "source": [
    "res = pred_class_modified(\"teach me olympus\",vect.vocabulary_,classes)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "mSTelRWPxkWn"
   },
   "outputs": [],
   "source": [
    "def getResponse(tag) :\n",
    "  df = finaldf[finaldf['tag']==tag]\n",
    "  if (df.shape[0] >=1) :\n",
    "    return df['response'].iloc[0:1]\n",
    "  else :\n",
    "    return \"Can you please change your question\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_dN5dcxJ_r7"
   },
   "outputs": [],
   "source": [
    "# running the chatbot\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    tags = pred_class_modified(message, vect.vocabulary_,classes)\n",
    "    result = getResponse(tags)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVXrNGl1IUIa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "trUuQ-RXhiF7"
   ],
   "name": "pgp_aiml_feb20_StatsNLP_R9_Project_Chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
