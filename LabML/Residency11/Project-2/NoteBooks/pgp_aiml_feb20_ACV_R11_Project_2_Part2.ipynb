{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPs64QA1Zdov"
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xk4FU-jx9kc3"
   },
   "outputs": [],
   "source": [
    "# This Colab requires TF 2.5.\n",
    "!pip install -U tensorflow>=2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yn5_uV1HLvaz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from six.moves.urllib.request import urlopen\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IogyryF2lFBL"
   },
   "source": [
    "## Utilities\n",
    "\n",
    "Run the following cell to create some utils that will be needed later:\n",
    "\n",
    "- Helper method to load an image\n",
    "- Map of Model Name to TF Hub handle\n",
    "- List of tuples with Human Keypoints for the COCO 2017 dataset. This is needed for models with keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y9R0Xllefec"
   },
   "outputs": [],
   "source": [
    "# @title Run this!!\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "  Puts image into numpy array to feed into tensorflow graph.\n",
    "  Note that by convention we put it into a numpy array with shape\n",
    "  (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "  Args:\n",
    "    path: the file path to the image\n",
    "\n",
    "  Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  image = None\n",
    "  if(path.startswith('http')):\n",
    "    response = urlopen(path)\n",
    "    image_data = response.read()\n",
    "    image_data = BytesIO(image_data)\n",
    "    image = Image.open(image_data)\n",
    "  else:\n",
    "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "ALL_MODELS = {\n",
    "'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
    "'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
    "'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n",
    "'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n",
    "'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
    "'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n",
    "'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n",
    "'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
    "'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n",
    "'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
    "'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
    "'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
    "'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
    "'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n",
    "'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n",
    "'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n",
    "'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n",
    "'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
    "'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n",
    "'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
    "'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n",
    "'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
    "'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
    "'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
    "'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
    "'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
    "'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n",
    "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
    "'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
    "'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
    "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
    "'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
    "'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
    "'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
    "'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
    "'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
    "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
    "'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
    "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
    "}\n",
    "\n",
    "IMAGES_FOR_TEST = {\n",
    "  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n",
    "  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n",
    "  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n",
    "  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n",
    "  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n",
    "  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n",
    "  # By Am√©rico Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n",
    "  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n",
    "  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n",
    "  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n",
    "  'veh1' : 'models/research/object_detection/test_images/veh1.jpg',\n",
    "  'twov1' : 'models/research/object_detection/test_images/twov1.jpg',\n",
    "  'twocars' :'/content/models/research/object_detection/test_images/twocars.jpg',\n",
    "  'carbus' :'/content/models/research/object_detection/test_images/carbus.jpg',\n",
    "\n",
    "}\n",
    "\n",
    "COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n",
    " (0, 2),\n",
    " (1, 3),\n",
    " (2, 4),\n",
    " (0, 5),\n",
    " (0, 6),\n",
    " (5, 7),\n",
    " (7, 9),\n",
    " (6, 8),\n",
    " (8, 10),\n",
    " (5, 6),\n",
    " (5, 11),\n",
    " (6, 12),\n",
    " (11, 12),\n",
    " (11, 13),\n",
    " (13, 15),\n",
    " (12, 14),\n",
    " (14, 16)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14bNk1gzh0TN"
   },
   "source": [
    "## Installating the Object Detection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi28cqGGFWnY"
   },
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository\n",
    "!git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwdsBdGhFanc"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt install -y protobuf-compiler\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "python -m pip install .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yDNgIx-kV7X"
   },
   "source": [
    "Import the dependencies we will need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JCeQU3fkayh"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKtD0IeclbL5"
   },
   "source": [
    "### Load label map data\n",
    "\n",
    "Label maps correspond index numbers to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine.\n",
    "\n",
    "We are going, for simplicity, to load from the repository that we loaded the Object Detection API code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mucYUS6exUJ"
   },
   "outputs": [],
   "source": [
    "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjn9Mz2BAs-5"
   },
   "outputs": [],
   "source": [
    "category_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6917xnUSlp9x"
   },
   "source": [
    "## Build a detection model and load pre-trained model weights\n",
    "\n",
    "Here we will choose which Object Detection model we will use.\n",
    "Select the architecture and it will be loaded automatically.\n",
    "If you want to change the model to try other architectures later, just change the next cell and execute following ones.\n",
    "\n",
    "**Tip:** if you want to read more details about the selected model, you can follow the link (model handle) and read additional documentation on TF Hub. After you select a model, we will print the handle to make it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtwrSqvakTNn"
   },
   "outputs": [],
   "source": [
    "#@title Model Selection { display-mode: \"form\", run: \"auto\" }\n",
    "model_display_name = 'SSD MobileNet V2 FPNLite 320x320' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n",
    "model_handle = ALL_MODELS[model_display_name]\n",
    "print('Selected model:'+ model_display_name)\n",
    "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muhUt-wWL582"
   },
   "source": [
    "## Loading the selected model from TensorFlow Hub\n",
    "\n",
    "Here we just need the model handle that was selected and use the Tensorflow Hub library to load it to memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBuD07fLlcEO"
   },
   "outputs": [],
   "source": [
    "print('loading model...')\n",
    "hub_model = hub.load(model_handle)\n",
    "print('model loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIawRDKPPnd4"
   },
   "source": [
    "## Loading an image\n",
    "\n",
    "Let's try the model on a simple image. To help with this, we provide a list of test images.\n",
    "\n",
    "Here are some simple things to try out if you are curious:\n",
    "* Try running inference on your own images, just upload them to colab and load the same way it's done in the cell below.\n",
    "* Modify some of the input images and see if detection still works.  Some simple things to try out here include flipping the image horizontally, or converting to grayscale (note that we still expect the input image to have 3 channels).\n",
    "\n",
    "**Be careful:** when using images with an alpha channel, the model expect 3 channels images and the alpha will count as a 4th.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hX-AWUQ1wIEr"
   },
   "outputs": [],
   "source": [
    "#@title Image Selection (don't forget to execute the cell!) { display-mode: \"form\"}\n",
    "selected_image = \"twocars\" #@param [\"Beach\", \"Dogs\", \"Naxos Taverna\", \"Beatles\", \"Phones\", \"Birds\", \"veh1\", \"twov1\", \"twocars\", \"carbus\"]\n",
    "flip_image_horizontally = False #@param {type:\"boolean\"}\n",
    "convert_image_to_grayscale = False #@param {type:\"boolean\"}\n",
    "\n",
    "image_path = IMAGES_FOR_TEST[selected_image]\n",
    "image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "# Flip horizontally\n",
    "if(flip_image_horizontally):\n",
    "  image_np[0] = np.fliplr(image_np[0]).copy()\n",
    "\n",
    "# Convert image to grayscale\n",
    "if(convert_image_to_grayscale):\n",
    "  image_np[0] = np.tile(\n",
    "    np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "#plt.figure(figsize=(24,32))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image_np[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVUpUfocFsRU"
   },
   "outputs": [],
   "source": [
    "# running inference\n",
    "results = hub_model(image_np)\n",
    "result = {key:value.numpy() for key,value in results.items()}\n",
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zk76gQ0cFvj4"
   },
   "outputs": [],
   "source": [
    "confidence_threshold = 0.4\n",
    "selected_predictions = result['detection_scores'] >= confidence_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcUO5Er-FyDP"
   },
   "outputs": [],
   "source": [
    "selected_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnwxUhT3F2ik"
   },
   "outputs": [],
   "source": [
    "unqiue,count = np.unique(selected_predictions[0],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxMaIF6bF-Vj"
   },
   "outputs": [],
   "source": [
    "print(unqiue,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQATXc5qF5s9"
   },
   "outputs": [],
   "source": [
    "result['detection_classes'][selected_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiMD0VctGKSX"
   },
   "outputs": [],
   "source": [
    "category_index[result['detection_classes'][selected_predictions][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVIPH3NWGRGY"
   },
   "outputs": [],
   "source": [
    "category_index[result['detection_classes'][selected_predictions][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Z6YgpXuGTbz"
   },
   "outputs": [],
   "source": [
    "category_index[result['detection_classes'][selected_predictions][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wJts8xSKWpg"
   },
   "outputs": [],
   "source": [
    " dic = category_index[result['detection_classes'][selected_predictions][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAxx3XviKev1"
   },
   "outputs": [],
   "source": [
    "dic['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiGcnwmiluKz"
   },
   "outputs": [],
   "source": [
    "result['detection_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_fzkQXPmwZj"
   },
   "outputs": [],
   "source": [
    "result['detection_classes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTzW0esZm4aY"
   },
   "outputs": [],
   "source": [
    "result['detection_boxes'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2X4Lujllfqt"
   },
   "outputs": [],
   "source": [
    "int(result['detection_boxes'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qld7LrVbp0oN"
   },
   "outputs": [],
   "source": [
    "selected_predictions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTUYKs1YmTrf"
   },
   "outputs": [],
   "source": [
    "lst =[2,3,4,5,6,7,8]\n",
    "boxlst =[]\n",
    "size = result['detection_classes'].shape[1]\n",
    "for num in range(size) :\n",
    "  #if result['detection_classes'][0][num] in lst :\n",
    "  if selected_predictions[0][num] and result['detection_classes'][0][num] in lst:\n",
    "    boxlst.append(result['detection_boxes'][0][num])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVsHr8AWozcO"
   },
   "outputs": [],
   "source": [
    "boxlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1QcrB-zrG-P"
   },
   "outputs": [],
   "source": [
    "boxlst[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssu-pTBmu2Ml"
   },
   "outputs": [],
   "source": [
    "detection_boxes = result['detection_boxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpBxHYLUu952"
   },
   "outputs": [],
   "source": [
    "selected_prediction_boxes = result['detection_boxes'][selected_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kamLMhKjvIx8"
   },
   "outputs": [],
   "source": [
    "selected_prediction_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RaP4-zLqvxX6"
   },
   "outputs": [],
   "source": [
    "  img_h, img_w = image_np.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDIP0OzPv-yj"
   },
   "outputs": [],
   "source": [
    "for i in range(selected_prediction_boxes.shape[0]):\n",
    "  selected_prediction_boxes[i,0] *= img_h #ymin * img_w\n",
    "  selected_prediction_boxes[i,1] *= img_w #xmin * img_h\n",
    "  selected_prediction_boxes[i,2] *= img_h #ymax * img_w\n",
    "  selected_prediction_boxes[i,3] *= img_w #xmax * img_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeoSvJtQwJb1"
   },
   "outputs": [],
   "source": [
    "selected_prediction_boxes= selected_prediction_boxes.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fG3_QsKEwRj4"
   },
   "outputs": [],
   "source": [
    "selected_prediction_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeE23EPCxMqp"
   },
   "outputs": [],
   "source": [
    "box = selected_prediction_boxes[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4HMpCXgxgRF"
   },
   "outputs": [],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LDHNgJdxSzh"
   },
   "outputs": [],
   "source": [
    "org = (0,img_h-10)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 1\n",
    "      # Red color in BGR\n",
    "      # Line thickness of 2 px\n",
    "thickness = 1\n",
    "color = (0, 0, 255)\n",
    "cv2.rectangle(image_np[0], (box[1], box[0]), (box[3], box[2]), (0,255,0), 2)\n",
    "      #cv2.rectangle(img, (xmin,ymin), (xmax, ymax), (0,255,0), 2)\n",
    "cv2.putText(image_np[0], 'Hello', org, font, fontScale, color, thickness, cv2.LINE_AA, False)\n",
    "img = cv2.cvtColor(image_np[0], cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqvqQiA3yPoU"
   },
   "outputs": [],
   "source": [
    "image_np[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diO2lu9txzd4"
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAEnHEYdyMkn"
   },
   "outputs": [],
   "source": [
    "selected_prediction_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzaBfusjulL0"
   },
   "outputs": [],
   "source": [
    "    detection_boxes = result['detection_boxes'][0]\n",
    "    selected_prediction_boxes = result['detection_boxes'][selected_predictions]\n",
    "    #De-normalize box co-ordinates (multiply x-coordinates by image width and y-coords by image height)\n",
    "    img_h, img_w = image_np.shape[1:3]\n",
    "\n",
    "    for i in range(selected_prediction_boxes.shape[0]):\n",
    "        \n",
    "        selected_prediction_boxes[i,0] *= img_h #ymin * img_w\n",
    "        selected_prediction_boxes[i,1] *= img_w #xmin * img_h\n",
    "        selected_prediction_boxes[i,2] *= img_h #ymax * img_w\n",
    "        selected_prediction_boxes[i,3] *= img_w #xmax * img_h\n",
    "    \n",
    "    #Make all co-ordinates as integer\n",
    "    selected_prediction_boxes= selected_prediction_boxes.astype(int)\n",
    "\n",
    "    lst =[2,3,4,5,6,7,8]\n",
    "    boxlst =[]\n",
    "    size = result['detection_classes'].shape[1]\n",
    "    for num in range(size) :\n",
    "      #if result['detection_classes'][0][num] in lst :\n",
    "      if selected_predictions[0][num] and result['detection_classes'][0][num] in lst:\n",
    "        boxlst.append(selected_prediction_boxes[num]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_deJpzTiyruS"
   },
   "outputs": [],
   "source": [
    "boxlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsW2DpZvxHcm"
   },
   "outputs": [],
   "source": [
    "      box = selected_prediction_boxes[i]      \n",
    "\n",
    "      org = (0,img_h-10)\n",
    "      font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "      fontScale = 1\n",
    "      # Red color in BGR\n",
    "      # Line thickness of 2 px\n",
    "      thickness = 1\n",
    "      color = (0, 0, 255)\n",
    "      frame = cv2.rectangle(img, (box[1], box[0]), (box[3], box[2]), (0,255,0), 2)\n",
    "      frame = cv2.putText(img, text, org, font, fontScale, color, thickness, cv2.LINE_AA, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOk58kCO0aJE"
   },
   "outputs": [],
   "source": [
    "    cv2.rectangle(img, (xmin,ymin), (xmax, ymax), (0,255,0), 2)\n",
    "    #cv2.rectangle(img, (int(xmin),int(ymin)), (int(xmax), int(ymax)), (0,255,0), 2)\n",
    "    #Add text\n",
    "    cv2.putText(img,label,(int(xmin), int(ymin)-5),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2SliQYJH9hU"
   },
   "outputs": [],
   "source": [
    "def NoofVehicles(arr) :\n",
    "  vehicleCount=0\n",
    "  #for item in result['detection_classes'][selected_predictions] :\n",
    "  for item in arr :\n",
    "    dic = category_index[int(item)]\n",
    "    #print(dic['id'])\n",
    "    \"\"\"\n",
    "      Lets search for below objects in the result and count it\n",
    "      2: {'id': 2, 'name': 'bicycle'},\n",
    "      3: {'id': 3, 'name': 'car'},\n",
    "      4: {'id': 4, 'name': 'motorcycle'},\n",
    "      5: {'id': 5, 'name': 'airplane'},\n",
    "      6: {'id': 6, 'name': 'bus'},\n",
    "      7: {'id': 7, 'name': 'train'},\n",
    "      8: {'id': 8, 'name': 'truck'},\n",
    "    \"\"\"\n",
    "    lst =[2,3,4,5,6,7,8]\n",
    "    if dic['id'] in lst :\n",
    "      vehicleCount = vehicleCount +  1   \n",
    "  \n",
    "  return vehicleCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_vfR1G9MkrF"
   },
   "outputs": [],
   "source": [
    "print(NoofVehicles(result['detection_classes'][selected_predictions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dFkNT-kMAkS"
   },
   "source": [
    "# Read video to count vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11xUAZ9CMTyb"
   },
   "outputs": [],
   "source": [
    "videopath ='/content/models/research/object_detection/test_images/cars.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbAFX189NVdx"
   },
   "outputs": [],
   "source": [
    "save_path = '/content/models/research/object_detection/test_images/vedioimages/output.webm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TLzJ_Z4NV21"
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0RtX9FLkKNU"
   },
   "source": [
    "# Logic building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qp7AF0GRBCV"
   },
   "outputs": [],
   "source": [
    "#Load video\n",
    "#capture = cv2.VideoCapture(videopath)\n",
    "#Get the first frame\n",
    "hasFrame, frame = capture.read()\n",
    "\n",
    "img_h = frame.shape[0]\n",
    "img_w = frame.shape[1]\n",
    "\n",
    "#if save_path given, initialize video writer\n",
    "if save_path:\n",
    "  _fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "  _out = cv2.VideoWriter(save_path, _fourcc, 25, (img_w,img_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IvBhvoIRS40"
   },
   "outputs": [],
   "source": [
    "_fourcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhIPBO9TRRMr"
   },
   "outputs": [],
   "source": [
    "img = frame\n",
    "frame = tf.reshape(frame, [1, img_h, img_w, 3])\n",
    "results = hub_model(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mygbshqdSe6v"
   },
   "outputs": [],
   "source": [
    "result = {key:value.numpy() for key,value in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOVhSdh8SQhA"
   },
   "outputs": [],
   "source": [
    "confidence_threshold = 0.4\n",
    "selected_predictions = result['detection_scores'] >= confidence_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwKGeN38Rnj7"
   },
   "outputs": [],
   "source": [
    "result['detection_classes'][selected_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "700B5hupTZRY"
   },
   "outputs": [],
   "source": [
    "vehCount = NoofVehicles(result['detection_classes'][selected_predictions])\n",
    "print(vehCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkTVH1U3zGVq"
   },
   "outputs": [],
   "source": [
    "detection_boxes = result['detection_boxes'][0]\n",
    "selected_prediction_boxes = result['detection_boxes'][selected_predictions]\n",
    "#De-normalize box co-ordinates (multiply x-coordinates by image width and y-coords by image height)\n",
    "img_h, img_w = img.shape[0:2]\n",
    "\n",
    "for i in range(selected_prediction_boxes.shape[0]):\n",
    "  selected_prediction_boxes[i,0] *= img_h #ymin * img_w\n",
    "  selected_prediction_boxes[i,1] *= img_w #xmin * img_h\n",
    "  selected_prediction_boxes[i,2] *= img_h #ymax * img_w\n",
    "  selected_prediction_boxes[i,3] *= img_w #xmax * img_h\n",
    "    \n",
    "#Make all co-ordinates as integer\n",
    "selected_prediction_boxes= selected_prediction_boxes.astype(int)\n",
    "\n",
    "lst =[2,3,4,5,6,7,8]\n",
    "boxlst =[]\n",
    "size = result['detection_classes'].shape[1]\n",
    "for num in range(size) :\n",
    "  #if result['detection_classes'][0][num] in lst :\n",
    "  if selected_predictions[0][num] and result['detection_classes'][0][num] in lst:\n",
    "    boxlst.append(selected_prediction_boxes[num]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UARg_x6V0KaW"
   },
   "outputs": [],
   "source": [
    "org = (0,img_h-10)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 0.2\n",
    "      # Red color in BGR\n",
    "      # Line thickness of 2 px\n",
    "thickness = 1\n",
    "color = (0, 0, 255)\n",
    "#cv2.rectangle(img, (boxlst[0][1], boxlst[0][0]), (boxlst[0][3], boxlst[0][2]), (0,255,0), 2)\n",
    "cv2.rectangle(img, (boxlst[0][1], boxlst[0][0]), (boxlst[0][3], boxlst[0][2]), (0,255,0), 2)\n",
    "      #cv2.rectangle(img, (xmin,ymin), (xmax, ymax), (0,255,0), 2)\n",
    "cv2.putText(img, 'Veh Count' + str(vehCount), org, font, fontScale, color, thickness, cv2.LINE_AA, False)\n",
    "#img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJAj-GJ7UnnO"
   },
   "outputs": [],
   "source": [
    "_out.write(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jZ5ExJPVEKk"
   },
   "outputs": [],
   "source": [
    "capture.release()\n",
    "#Close the output video file\n",
    "if save_path:\n",
    "  _out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCMycqSlkRD1"
   },
   "source": [
    "# Check No fo Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPleyZN0cOKo"
   },
   "outputs": [],
   "source": [
    "#Load video\n",
    "capture = cv2.VideoCapture(videopath)\n",
    "#Get the first frame\n",
    "hasFrame, frame = capture.read()\n",
    "counter1=0\n",
    "img_h = frame.shape[0]\n",
    "img_w = frame.shape[1]\n",
    "\n",
    "#if save_path given, initialize video writer\n",
    "if save_path:\n",
    "  _fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "  _out = cv2.VideoWriter(save_path, _fourcc, 60, (img_w,img_h))\n",
    "\n",
    "while hasFrame :\n",
    "  try :\n",
    "    #frame =np.array(frame.getdata()).reshape((1, im_height, im_width, 3)).astype(np.uint8)\n",
    "    counter1= counter1+1\n",
    "    hasFrame, frame = capture.read()\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    hasFrame, frame = capture.read()\n",
    "\n",
    "#cv2.destroyAllWindows()\n",
    "capture.release()\n",
    "#Close the output video file\n",
    "if save_path:\n",
    "  _out.release()\n",
    "\n",
    "print(\"No of frames in Video-\",counter1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bxx0EBckXCy"
   },
   "source": [
    "# Read vedio and count vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqJkv_GCKCL_"
   },
   "source": [
    "1st File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN59ZMR48y9t"
   },
   "outputs": [],
   "source": [
    "videopath ='/content/models/research/object_detection/test_images/video2.mp4'\n",
    "#save_path = '/content/models/research/object_detection/test_images/vedioimages/output2.webm'\n",
    "save_path = '/content/models/research/object_detection/test_images/vedioimages/video2output.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMlOOqGJKEbQ"
   },
   "source": [
    "2nd File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z206_YGVKHI_"
   },
   "outputs": [],
   "source": [
    "videopath ='/content/models/research/object_detection/test_images/cars.mp4'\n",
    "#save_path = '/content/models/research/object_detection/test_images/vedioimages/output2.webm'\n",
    "save_path = '/content/models/research/object_detection/test_images/vedioimages/carsoutput.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzTanoZhL_Iy"
   },
   "outputs": [],
   "source": [
    "#Load video\n",
    "capture = cv2.VideoCapture(videopath)\n",
    "#Get the first frame\n",
    "hasFrame, frame = capture.read()\n",
    "\n",
    "img_h = frame.shape[0]\n",
    "img_w = frame.shape[1]\n",
    "\n",
    "#if save_path given, initialize video writer\n",
    "if save_path:\n",
    "  #_fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "  _fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "  _out = cv2.VideoWriter(save_path, _fourcc, 25, (img_w,img_h))\n",
    "\n",
    "while hasFrame :\n",
    "  try :\n",
    "    #frame =np.array(frame.getdata()).reshape((1, im_height, im_width, 3)).astype(np.uint8)\n",
    "    img = frame\n",
    "    frame = tf.reshape(frame, [1, img_h, img_w, 3])\n",
    "    results = hub_model(frame)\n",
    "    result = {key:value.numpy() for key,value in results.items()}\n",
    "    confidence_threshold = 0.4\n",
    "    selected_predictions = result['detection_scores'] >= confidence_threshold\n",
    "    vehCount = NoofVehicles(result['detection_classes'][selected_predictions])\n",
    "    #Write no of veicle count in the image at the left-bottom corner\n",
    "    text = \"Veh No\" + str(vehCount)\n",
    "\n",
    "    detection_boxes = result['detection_boxes'][0]\n",
    "    selected_prediction_boxes = result['detection_boxes'][selected_predictions]\n",
    "    #De-normalize box co-ordinates (multiply x-coordinates by image width and y-coords by image height)\n",
    "    img_h, img_w = img.shape[0:2]\n",
    "    \n",
    "    for i in range(selected_prediction_boxes.shape[0]):\n",
    "      selected_prediction_boxes[i,0] *= img_h #ymin * img_w\n",
    "      selected_prediction_boxes[i,1] *= img_w #xmin * img_h\n",
    "      selected_prediction_boxes[i,2] *= img_h #ymax * img_w\n",
    "      selected_prediction_boxes[i,3] *= img_w #xmax * img_h\n",
    "    \n",
    "    #Make all co-ordinates as integer\n",
    "    selected_prediction_boxes= selected_prediction_boxes.astype(int)\n",
    "\n",
    "    lst =[2,3,4,5,6,7,8] # Classes for vehicles\n",
    "    boxlst =[]\n",
    "    size = result['detection_classes'].shape[1]\n",
    "    for num in range(size) :\n",
    "      if selected_predictions[0][num] and result['detection_classes'][0][num] in lst:\n",
    "        boxlst.append(selected_prediction_boxes[num])\n",
    "\n",
    "    if len(boxlst) > 0 :\n",
    "      for box in boxlst :\n",
    "        org = (0,img_h-10)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        fontScale = 1\n",
    "        # Red color in BGR\n",
    "        # Line thickness of 2 px\n",
    "        thickness = 1\n",
    "        color = (0, 0, 255)\n",
    "        cv2.rectangle(img, (box[1], box[0]), (box[3], box[2]), (0,255,0), 2)\n",
    "        #cv2.rectangle(img, (xmin,ymin), (xmax, ymax), (0,255,0), 2)\n",
    "        cv2.putText(img, text, org, font, fontScale, color, thickness, cv2.LINE_AA, False)\n",
    "\n",
    "      #frame = cv2.rectangle(img, (box[1], box[0]), (box[3], box[2]), (0,255,0), 2)\n",
    "      #frame = cv2.putText(img, text, org, font, fontScale, color, thickness, cv2.LINE_AA, False)\n",
    "\n",
    "      #Save the output file\n",
    "    if save_path:\n",
    "      #_out.write(frame)\n",
    "      _out.write(img)\n",
    "    #Read the next frame\n",
    "    hasFrame, frame = capture.read()\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    hasFrame, frame = capture.read()\n",
    "\n",
    "#cv2.destroyAllWindows()\n",
    "capture.release()\n",
    "#Close the output video file\n",
    "if save_path:\n",
    "  _out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O62fwOkIbQga"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCuOQl0obSn1"
   },
   "outputs": [],
   "source": [
    "video_file = open('/content/models/research/object_detection/test_images/vedioimages/output.webm','rb').read()\n",
    "data_url = \"data:video/webm;base64,\" + b64encode(video_file).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlOgG9ynbYNd"
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video controls>\n",
    "      <source src=\"%s\" type=\"video/webm\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pgp_aiml_feb20_ACV_R11_Project_2_Part2_V3.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
