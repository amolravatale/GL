{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Audio Data - UrbanSound8K.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMrgbe088uvdWVj9IsRSBbB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4K3Dxpg9tYZ9"},"source":["#### Download Dataset\n","\n","UrbanSound8K -> [Link](https://urbansounddataset.weebly.com/download-urbansound8k.html)"]},{"cell_type":"code","metadata":{"id":"TQOB09P4qB3w"},"source":["#Download data using the link below\n","!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LjX2JCvx1Oit"},"source":["#Confirm data has been downloaded\n","!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AmGLzRhscGy"},"source":["#Unzip file\n","!tar -xvf UrbanSound8K.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NX7Q7Wy52UfM"},"source":["#### Load Dataset"]},{"cell_type":"code","metadata":{"id":"KvM9BMROtLW4"},"source":["import pandas as pd\n","import os\n","import librosa\n","import librosa.display\n","import numpy as np\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOEtf9HttOsj"},"source":["#Read CSV file\n","df = pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n","print('Number of audio files:',df.shape[0])\n","df.sample(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aKPhaDbKu446"},"source":["#Number of unique classes\n","df['class'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rSYs7DgE5PNF"},"source":["#Class labels\n","df['classID'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8vD6-T5wfU3"},"source":["### Extract Audio Features"]},{"cell_type":"markdown","metadata":{"id":"GX6Crz127rup"},"source":["Read an audio file"]},{"cell_type":"code","metadata":{"id":"-vOSeEBiwRcj"},"source":["#We will use librosa package to load the audio files. \n","#Files are sampled at 22.05 KHz and are always converted to mono sound\n","\n","idx = np.random.randint(0, df.shape[0])\n","file_name = 'UrbanSound8K/audio/fold' + str(df.loc[idx, \"fold\"]) +'/' + df.loc[idx,\"slice_file_name\"]\n","print(file_name)\n","audio, sample_rate = librosa.load(file_name)\n","\n","print('Sample rate:', sample_rate)\n","print('Audio array shape:', audio.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AoMkOh3O6mu6"},"source":["audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G9xpeubv7yo9"},"source":["Visualize an audio signal"]},{"cell_type":"code","metadata":{"id":"mFItqEVQ7xei"},"source":["idx = np.random.randint(0, df.shape[0])\n","file_name = 'UrbanSound8K/audio/fold' + str(df.loc[idx, \"fold\"]) +'/' + df.loc[idx,\"slice_file_name\"]\n","audio, sample_rate = librosa.load(file_name)\n","librosa.display.waveplot(audio, sr= sample_rate)\n","plt.suptitle(df.loc[idx, 'class'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nryFJT0h3gDq"},"source":["Playing an audio"]},{"cell_type":"code","metadata":{"id":"-7to3a4d10Bq"},"source":["!pip3 install pydub --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKu2c2Hq2dcQ"},"source":["from pydub import AudioSegment\n","from pydub.playback import play\n","\n","sound = AudioSegment.from_wav(file_name)\n","play(sound)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wTIX4mzW78Cu"},"source":["Get MFCCs (Mel frequency Cepstral Coefficients) feature"]},{"cell_type":"code","metadata":{"id":"ZCfWP-on7-TI"},"source":["mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n","mfccsscaled = np.mean(mfccs.T,axis=0)\n","print(mfccs.shape, mfccsscaled.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbTEr1y-7Vbb"},"source":["mfccsscaled"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LkqJ4ew48OMk"},"source":["Function to extract MFCC feature for each audio file"]},{"cell_type":"code","metadata":{"id":"0AJvRlVu8HR_"},"source":["def extract_mfcc_feature(idx):\n","\n","    global df\n","\n","    try:\n","\n","        #Sample audio signal\n","        file_name = 'UrbanSound8K/audio/fold' + str(df.loc[idx, \"fold\"]) + '/' + df.loc[idx,\"slice_file_name\"]\n","        audio, sample_rate = librosa.load(file_name)\n","\n","        #Convert to MFCC feature\n","        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n","        mfccsscaled = np.mean(mfccs.T,axis=0)\n","\n","        return mfccsscaled\n","    \n","    except Exception as e:\n","        print(e)\n","        return None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z60b2XN588iN"},"source":["#Get features for all audio signals\n","features = []\n","\n","#Selecting few examples\n","ids = np.random.randint(0, df.shape[0], df.shape[0])\n","\n","for i in ids:\n","\n","    data = extract_mfcc_feature(i)\n","\n","    if data is not None:\n","\n","        features.append([data, df.loc[i, 'classID']])\n","\n","#Create a dataframe for easier data handling\n","audio_df =  pd.DataFrame(features, columns=['Features', 'Label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N9h065Ge-QrI"},"source":["#Review audio features\n","print(audio_df.shape)\n","audio_df.sample(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PLi7xDZK-a7i"},"source":["Split data between training and test"]},{"cell_type":"code","metadata":{"id":"oQhadW3I_ADw"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZBQrzad-dWl"},"source":["from sklearn.model_selection import train_test_split "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5NHjjndGFuaF"},"source":["#Get X and Y as numpy array\n","X = np.array(audio_df['Features'].tolist())\n","y = np.array(audio_df['Label'].tolist())\n","\n","#One hot encoding of Label\n","y = tf.keras.utils.to_categorical(y, num_classes=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P64VYtj2-swy"},"source":["trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.2, random_state = 42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTQKKYp2Ffto"},"source":["trainX.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKbsKAW_-7D-"},"source":["testX.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jQPDI5ip-9sj"},"source":["### Building Model"]},{"cell_type":"code","metadata":{"id":"7aOOarDy_Cce"},"source":["tf.keras.backend.clear_session()\n","model = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qqEu4b12_H7w"},"source":["model.add(tf.keras.layers.Reshape((40,1,), input_shape=(40,)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3sraiDrZ_PW8"},"source":["model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Conv1D(16, kernel_size=(3), activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Conv1D(32, (3), activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Conv1D(64, (3), activation='relu'))\n","model.add(tf.keras.layers.GlobalAveragePooling1D())\n","model.add(tf.keras.layers.Dense(10, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z6uHG83S_5R3"},"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UgRDkvsUH9-z"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-v47QKMYAmVH"},"source":["model.fit(trainX, trainY, epochs=200, validation_data=(testX, testY))"],"execution_count":null,"outputs":[]}]}