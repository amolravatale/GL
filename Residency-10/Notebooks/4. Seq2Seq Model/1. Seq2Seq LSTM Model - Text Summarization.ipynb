{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Seq2Seq LSTM Model - Text Summarization.ipynb","provenance":[],"private_outputs":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5csO-wPPKXmw","colab_type":"text"},"source":["#### Connect to Kaggle\n","\n","Data is available on Kaggle website. We will first connect Colab to Kaggle. Instructions for downloading kaggle data to Colab can be found [in this post](https://towardsdatascience.com/setting-up-kaggle-in-google-colab-ebb281b61463)."]},{"cell_type":"code","metadata":{"id":"_NQWRPuoN_Ok","colab_type":"code","colab":{}},"source":["!pip install kaggle --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0ttJZT3KXmx","colab_type":"code","colab":{}},"source":["#Make a directory for Kaggle\n","!mkdir .kaggle\n","\n","#Connect Google drive to colab\n","from google.colab import drive\n","drive.mount('/gdrive')\n","\n","#Copy kaggle.json file. Change gdrive folder based on where you have saved your json file from Kaggle\n","!cp '/gdrive/My Drive/AI-ML/Machine-Learning/Code/Utilities/kaggle.json' /content/.kaggle/kaggle.json\n","\n","#Check if json file is there\n","!ls -l /content/.kaggle\n","\n","!mkdir ~/.kaggle\n","!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n","!kaggle config set -n path -v{/content}\n","!chmod 600 /root/.kaggle/kaggle.json"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80Pcvzu4KXmz","colab_type":"text"},"source":["#### Download Amazon Reviews Dataset\n","\n","Here is the [link](https://www.kaggle.com/snap/amazon-fine-food-reviews) for Amazon Fine Food reviews dataset. You may need to agree to the rules of the competition before download is allowed."]},{"cell_type":"code","metadata":{"id":"vQBFJRquKXm0","colab_type":"code","colab":{}},"source":["!kaggle datasets download -d snap/amazon-fine-food-reviews -p /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPY-rul5cgwp","colab_type":"code","colab":{}},"source":["!unzip amazon-fine-food-reviews.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4V6ZdyClsfE","colab_type":"code","colab":{}},"source":["!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0FZmY5HaOmIF","colab_type":"text"},"source":["#### Read Data"]},{"cell_type":"code","metadata":{"id":"JtMmO7f0aJdg","colab_type":"code","colab":{}},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iWfFrrcmcjmJ","colab_type":"code","colab":{}},"source":["df = pd.read_csv('Reviews.csv', nrows=50000)\n","df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QuTZABCz9qJC","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2l-nZHEJO4PG","colab_type":"code","colab":{}},"source":["#Drop NA\n","df.dropna(axis=0,inplace=True)\n","df.reset_index(inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5VKMQRn9a3vE","colab_type":"code","colab":{}},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ULej1R2BfW9n","colab_type":"text"},"source":["Review paragraph and summary"]},{"cell_type":"code","metadata":{"id":"o1rfGKCXe8Di","colab_type":"code","colab":{}},"source":["#Select a random record\n","rec_num = np.random.randint(0, df.shape[0])\n","print('--------')\n","print ('Paragraph: ', df.loc[rec_num, 'Text'])\n","print ('Summary: ', df.loc[rec_num, 'Summary'] )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnEw_IMEKXm_","colab_type":"text"},"source":["### Separate Source and Target pairs"]},{"cell_type":"code","metadata":{"id":"yQMiDL45KXnA","colab_type":"code","colab":{}},"source":["encoder_text = [] #Initialize paragraph list\n","decoder_text = [] #Initialize summary list\n","\n","for i in range(df.shape[0]):\n","\n","    #Get Pragraph and summary for each record\n","    text_paragraph = df.loc[i, 'Text']\n","    text_summary = df.loc[i, 'Summary']\n","\n","    #Add Paragraph to the list\n","    encoder_text.append(text_paragraph)\n","    #Add start and end sequence to summary text.\n","    #IMPORTANT: Make sure start_seq and end_seq are not part of regular words\n","    decoder_text.append('startseq ' + text_summary + ' endseq')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JdOMuGbUKXnC","colab_type":"text"},"source":["### Separate Source and Target pairs.."]},{"cell_type":"code","metadata":{"id":"7K8QDZAbKXnD","colab_type":"code","colab":{}},"source":["#Review some paragraphs\n","encoder_text[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DduIsYLKXnF","colab_type":"code","colab":{}},"source":["#Corresponding Summary Text\n","decoder_text[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1BA2RbOVKXnI","colab_type":"text"},"source":["### Tokenize Text data"]},{"cell_type":"code","metadata":{"id":"j6epKFDZQcqE","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2OiGqsXDKXnI","colab_type":"code","colab":{}},"source":["#Build Tokenizer\n","vocab_size=30000\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n","\n","#Fit both paragraph and summary text\n","tokenizer.fit_on_texts(encoder_text)\n","tokenizer.fit_on_texts(decoder_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8fZ-qAJ96Sw","colab_type":"code","colab":{}},"source":["print(tokenizer.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UgM11oD9b1kI","colab_type":"code","colab":{}},"source":["#Convert paragraph text to indexes\n","encoder_seq = tokenizer.texts_to_sequences(encoder_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cN5DneFYp0nc","colab_type":"code","colab":{}},"source":["encoder_text[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3c9RPZHOm7FR","colab_type":"code","colab":{}},"source":["print(encoder_seq[100:105]) #Display some converted sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQ5FlkVPRmja","colab_type":"code","colab":{}},"source":["#Convert summary text to indexes\n","decoder_seq = tokenizer.texts_to_sequences(decoder_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kyFL1wvpKXnL","colab_type":"code","colab":{}},"source":["#Maximum length of paragraph\n","max_encoder_seq_length = max([len(txt) for txt in encoder_seq])\n","\n","#Maximum length of summaryparagraph\n","max_decoder_seq_length = max([len(txt) for txt in decoder_seq])\n","\n","print('Maximum sentence length for paragraph: ', max_encoder_seq_length)\n","print('Maximum sentence length for summary: ', max_decoder_seq_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hi_fC8V3KXnR","colab_type":"text"},"source":["### Compare different sentences length"]},{"cell_type":"code","metadata":{"id":"XaoE7nULKXnS","colab_type":"code","colab":{}},"source":["#Source Language sentences\n","print('Length for sentence number 100: ', len(encoder_seq[100]))\n","print('Length for sentence number 150: ', len(encoder_seq[150]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFRd3s8XKXnV","colab_type":"code","colab":{}},"source":["#Target Language sentences\n","print('Length for sentence number 100: ', len(decoder_seq[100]))\n","print('Length for sentence number 150: ', len(decoder_seq[150]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-IatvBFKXnY","colab_type":"text"},"source":["### How do we make it same?"]},{"cell_type":"markdown","metadata":{"id":"WNZ8AeLKKXnZ","colab_type":"text"},"source":["### Padding the sentences"]},{"cell_type":"code","metadata":{"id":"ZfKDl8IAKXna","colab_type":"code","colab":{}},"source":["#Source sentences\n","encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(encoder_seq, \n","                                                                   maxlen=500,\n","                                                                   padding='pre', \n","                                                                   truncating='post')\n","\n","#Target Sentences\n","decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_seq, \n","                                                                   maxlen=max_decoder_seq_length,\n","                                                                   padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLTwqyLyKXnc","colab_type":"code","colab":{}},"source":["print('Source data shape: ', encoder_input_data.shape)\n","print('Target data shape: ', decoder_input_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LHIXCliIKXne","colab_type":"code","colab":{}},"source":["encoder_text[100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgBriKXSKXnf","colab_type":"code","colab":{}},"source":["encoder_input_data[100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJMPcCflKXnh","colab_type":"code","colab":{}},"source":["decoder_text[100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QokbLugoKXnj","colab_type":"code","colab":{}},"source":["decoder_input_data[100]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZoH9-bJKXnk","colab_type":"text"},"source":["#### Integer to Word converter for Decoder data"]},{"cell_type":"code","metadata":{"id":"IuoP4YMeKXnl","colab_type":"code","colab":{}},"source":["int_to_word_decoder = dict((i,c) for c, i in tokenizer.word_index.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVOpq75gKXnm","colab_type":"code","colab":{}},"source":["print(int_to_word_decoder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0NKxZeerVDu1","colab_type":"text"},"source":["#### Build Batch Generator"]},{"cell_type":"code","metadata":{"id":"t3SoSvUyV4LJ","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bSkjB_S2VGLY","colab_type":"code","colab":{}},"source":["def batch_generator(encoder_input_data, decoder_input_data, batch_size=64):\n","\n","    while True:\n","\n","        batch_idx = np.random.randint(0, encoder_input_data.shape[0], batch_size)\n","        paragraph_batch = np.zeros((batch_size, 500))\n","        summary_batch = np.zeros((batch_size, max_decoder_seq_length))\n","        decoder_output = np.zeros((batch_size, max_decoder_seq_length, vocab_size+1))\n","\n","        for i in range(batch_size):\n","\n","            paragraph_batch[i] = encoder_input_data[i]\n","            summary_batch[i] = decoder_input_data[i]\n","\n","            decoder_output_seq = np.zeros((max_decoder_seq_length))\n","            for j in range(1,max_decoder_seq_length):\n","                decoder_output_seq[j-1] = summary_batch[i][j]\n","\n","            for j in range(max_decoder_seq_length):\n","                decoder_output[i][j] = tf.keras.utils.to_categorical(decoder_output_seq[j],\n","                                                                     num_classes=vocab_size+1)\n","            \n","\n","        yield [paragraph_batch, summary_batch], decoder_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3ttrOHgYAqt","colab_type":"code","colab":{}},"source":["a = batch_generator(encoder_input_data, decoder_input_data, batch_size=2)\n","b,c = next(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JmFfF54WYP4x","colab_type":"code","colab":{}},"source":["c.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q6BNPWZ_KXoD","colab_type":"text"},"source":["### Building the Training Model"]},{"cell_type":"code","metadata":{"id":"q05uAVLXKXoD","colab_type":"code","colab":{}},"source":["#Define config parameters\n","encoder_embedding_size = 50\n","decoder_embedding_size = 50\n","rnn_units = 256 #Memory size for LSTM"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9M9eJGmSKXoF","colab_type":"text"},"source":["#### Build Encoder"]},{"cell_type":"code","metadata":{"id":"iJIqwk7aKkJ9","colab_type":"code","colab":{}},"source":["tf.keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5P1hzBqKXoG","colab_type":"code","colab":{}},"source":["#Input Layer\n","encoder_inputs = tf.keras.layers.Input(shape=(None,))\n","\n","#Embedding layer - Word2Vec \n","encoder_embedding = tf.keras.layers.Embedding(vocab_size+1, #Size for One hot encoding\n","                                              encoder_embedding_size) #How many numbers to use for each word\n","\n","#Get embedding layer output by feeding inputs\n","encoder_embedding_output = encoder_embedding(encoder_inputs)\n","\n","#LSTM Layer and its output\n","x, state_h, state_c = tf.keras.layers.LSTM(rnn_units,return_state=True)(encoder_embedding_output)\n","\n","#Build a list to feed Decoder - Sentence Embedding\n","encoder_states = [state_h, state_c]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrcqxcfhKXoH","colab_type":"code","colab":{}},"source":["encoder_states"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJvG6HRdKXoM","colab_type":"text"},"source":["#### Build Decoder"]},{"cell_type":"code","metadata":{"id":"3LQEt7zPKXoM","colab_type":"code","colab":{}},"source":["#Decode input - padded Target sentences\n","decoder_inputs = tf.keras.layers.Input(shape=(None,))\n","\n","#Decoder Embedding layer\n","decoder_embedding = tf.keras.layers.Embedding(vocab_size + 1, \n","                                              decoder_embedding_size)\n","\n","#Embedding layer output\n","decoder_embedding_output = decoder_embedding(decoder_inputs)\n","\n","#Decoder RNN\n","decoder_rnn = tf.keras.layers.LSTM(rnn_units, \n","                                   return_sequences=True, \n","                                   return_state=True)\n","\n","#Decoder RNN Output, State initialization from Encoder states\n","#Output will be all hidden sequences, last 'h' state and last 'c' state\n","x,_,_ = decoder_rnn(decoder_embedding_output, \n","                    initial_state=encoder_states)\n","\n","#Output Layer\n","decoder_dense = tf.keras.layers.Dense(vocab_size + 1, #+1 to make sure one-hot encoding works for highest index value\n","                                      activation='softmax')\n","\n","#Output of Dense layer\n","decoder_outputs = decoder_dense(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kabGUQ4KXoT","colab_type":"text"},"source":["### Build Model using both Encoder and Decoder"]},{"cell_type":"code","metadata":{"id":"RkJyiM7uKXoT","colab_type":"code","colab":{}},"source":["#Build a Seq2Seq model -> Encoder + Decoder\n","model = tf.keras.models.Model([encoder_inputs, decoder_inputs], #2 Inputs to the model\n","                              decoder_outputs) #Output of the model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5IvOy0tKXoW","colab_type":"code","colab":{}},"source":["model.output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-Yfmp3AKXoY","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam', loss='categorical_crossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQ4npEHA3xd4","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I30VxT7LKXoa","colab_type":"text"},"source":["### Train the model"]},{"cell_type":"code","metadata":{"id":"KGFW92rcY_By","colab_type":"code","colab":{}},"source":["train_generator = batch_generator(encoder_input_data, decoder_input_data, batch_size=64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-uw8R3_KXoa","colab_type":"code","colab":{}},"source":["model.fit(train_generator,\n","          steps_per_epoch = encoder_input_data.shape[0]//64,\n","          batch_size=64,\n","          epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDZlu731KXob","colab_type":"text"},"source":["Save the model"]},{"cell_type":"code","metadata":{"id":"jpUOjtksKXoc","colab_type":"code","colab":{}},"source":["model.save('seq2seq_text_summarization.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aGFZHOTZdKcY","colab_type":"code","colab":{}},"source":["!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"guuaYKaDKXod","colab_type":"text"},"source":["#### Building Model for Prediction"]},{"cell_type":"markdown","metadata":{"id":"j-tXkDaqKXod","colab_type":"text"},"source":["##### Build the Encoder Model to predict Encoder States"]},{"cell_type":"code","metadata":{"id":"QdM2QenRKXod","colab_type":"code","colab":{}},"source":["encoder_model = tf.keras.models.Model(encoder_inputs, #Padded input sequences\n","                                      encoder_states) #Hidden state and Cell state at last time step"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jj07QPsc941I","colab_type":"code","colab":{}},"source":["encoder_model.output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tBvy3uDnKXof","colab_type":"text"},"source":["##### Build the Decoder Model \n","<p/>\n","\n","<ol><li>Define Input for both 'h' state and 'c' state initialization </li>\n","<li>Get Decoder RNN outputs along with h and c state</li>\n","<li>Get Decoder Dense layer output</li>\n","        <li>Build Model</li></ol>"]},{"cell_type":"markdown","metadata":{"id":"Ohsoa5J_KXof","colab_type":"text"},"source":["##### Step 1 - Define Input for both 'h' state and 'c' state initialization"]},{"cell_type":"code","metadata":{"id":"aNlm9ufvKXoh","colab_type":"code","colab":{}},"source":["#Hidden state input\n","decoder_state_input_h = tf.keras.layers.Input(shape=(rnn_units,))\n","\n","#Cell state input\n","decoder_state_input_c = tf.keras.layers.Input(shape=(rnn_units,))\n","\n","#Putting it together\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZzbTg44oKXoi","colab_type":"text"},"source":["##### Step 2 - Get Decoder RNN outputs along with h and c state"]},{"cell_type":"code","metadata":{"id":"OVHzmr1hKXoi","colab_type":"code","colab":{}},"source":["#Get Embedding layer output\n","x = decoder_embedding(decoder_inputs)\n","\n","#We will use the layer which we trained earlier\n","rnn_outputs, state_h, state_c = decoder_rnn(x, initial_state=decoder_states_inputs)\n","\n","#Why do we need this?\n","decoder_states = [state_h, state_c]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1gX884-sKXoj","colab_type":"text"},"source":["##### Step 3 - Get Decoder Dense layer output"]},{"cell_type":"code","metadata":{"id":"lj9ZkJh0KXoj","colab_type":"code","colab":{}},"source":["decoder_outputs = decoder_dense(rnn_outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0HiEPMqnKXok","colab_type":"text"},"source":["##### Step 4 - Build Decoder Model"]},{"cell_type":"code","metadata":{"id":"ExV0PWSPKXol","colab_type":"code","colab":{}},"source":["decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_states_inputs,  #Model inputs\n","                                      [decoder_outputs] + decoder_states)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qnKbxenGKXom","colab_type":"text"},"source":["#### Predicting output from Seq2Seq model"]},{"cell_type":"markdown","metadata":{"id":"MFsy3KR7KXon","colab_type":"text"},"source":["Build a prediction function"]},{"cell_type":"code","metadata":{"id":"sIwVWxXXKXon","colab_type":"code","colab":{}},"source":["tokenizer.word_index['startseq']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nR2H6c3YKXop","colab_type":"code","colab":{}},"source":["int_to_word_decoder[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VEG8fmnbKXop","colab_type":"code","colab":{}},"source":["def summarize_paragraph(input_sequence):\n","    \n","    #Get the encoder state values - Sentence embedding\n","    decoder_initial_states_value = encoder_model.predict(input_seq)\n","    \n","    #Build a sequence with 'startseq' - starting sequence for Decoder\n","    target_seq = np.zeros((1,1))    \n","    target_seq[0][0] = tokenizer.word_index['startseq']\n","    \n","    #flag to check if prediction should be stopped\n","    stop_loop = False\n","    \n","    #Initialize predicted sentence\n","    predicted_sentence = ''\n","    \n","    num_of_predictions = 0\n","    \n","    #start the loop\n","    while not stop_loop:\n","        \n","        predicted_outputs, h, c = decoder_model.predict([target_seq] + \n","                                                        decoder_initial_states_value)\n","        \n","        #Get the predicted word index with highest probability\n","        predicted_output = np.argmax(predicted_outputs[0,-1,:])\n","        \n","        #Get the predicted word from predicter index\n","        predicted_word = int_to_word_decoder[predicted_output]\n","        \n","        #Check if prediction should stop\n","        if(predicted_word == 'endseq' or num_of_predictions > max_decoder_seq_length):\n","            \n","            stop_loop = True\n","            continue\n","        \n","        num_of_predictions += 1\n","        \n","        #Updated predicted sentence\n","        if (len(predicted_sentence) == 0):\n","            predicted_sentence = predicted_word\n","        else:\n","            predicted_sentence = predicted_sentence + ' ' + predicted_word\n","            \n","        #Update target_seq to be the predicted word index\n","        target_seq[0][0] = predicted_output\n","        \n","        #Update initial states value for decoder\n","        decoder_initial_states_value = [h,c]\n","        \n","    \n","    return predicted_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_jtZM8iaKXoq","colab_type":"text"},"source":["##### Call Prediction function on a random sentence"]},{"cell_type":"code","metadata":{"id":"perGUle1KXor","colab_type":"code","colab":{}},"source":["#Generate a random number\n","start_num = np.random.randint(0, high=len(encoder_text) - 10)\n","\n","#Predict model output for 5 sentences\n","for i in range(start_num, start_num + 5):\n","    input_seq = encoder_input_data[i : i+1]\n","    predicted_summary = summarize_paragraph(input_seq)\n","    print('--------')\n","    print ('Input paragraph: ', encoder_text[i])\n","    print ('Predicted summary: ', predicted_summary )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xqi_-4Y0KXos","colab_type":"text"},"source":["##### Save encoder and decoder model"]},{"cell_type":"code","metadata":{"id":"1TbJoYhJKXos","colab_type":"code","colab":{}},"source":["#Compile models to avoid error\n","encoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n","decoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n","\n","#Save the models\n","encoder_model.save('seq2seq_encoder_eng_hin.hd5')  #Encoder model\n","decoder_model.save('seq2seq_decoder_eng_hin.hd5')  #Decoder model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"skLA1matKXot","colab_type":"text"},"source":["##### Save Tokenizer"]},{"cell_type":"code","metadata":{"id":"eCLkBUL7KXou","colab_type":"code","colab":{}},"source":["import pickle\n","\n","pickle.dump(tokenizer,open('tokenizer_summarize','wb'))"],"execution_count":null,"outputs":[]}]}